{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "402ebf87-fdde-4d4f-940f-861e7c244ef9",
   "metadata": {},
   "source": [
    "### Importing the imdb dataset by stanford"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8795a456-fe98-4f4a-84ce-08360134364f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38314dc8-083f-4cbf-bfe3-3259ba679f6c",
   "metadata": {},
   "source": [
    "#### Below is the code for preprocessing, Since according to syntax, we require the input text to be a string (whole document is a single string) for Bag of Words and for TF-IDF hence the step 7 mentioned below, when performing encoding, embedding, syntactially we require each word as seperately hence will execute those parts seperately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc6a2873-883b-411d-8ed2-3a9003582902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading NLTK data...\n",
      "NLTK data downloaded.\n",
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /Users/vihaa/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- BEFORE PREPROCESSING ---\n",
      "I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn't have much of a plot.\n",
      "\n",
      "Applying preprocessing to dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "448a3cc20e3b4dabb29a4f753d3ea3ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- AFTER PREPROCESSING ---\n",
      "rented curiousyellow video store controversy surrounded first released also heard first seized u custom ever tried enter country therefore fan film considered controversial really see plot centered around young swedish drama student named lena want learn everything life particular want focus attention making sort documentary average swede thought certain political issue vietnam war race issue united state asking politician ordinary denizen stockholm opinion politics sex drama teacher classmate married men kill curiousyellow year ago considered pornographic really sex nudity scene far even shot like cheaply made porno countryman mind find shocking reality sex nudity major staple swedish cinema even ingmar bergman arguably answer good old boy john ford sex scene film commend filmmaker fact sex shown film shown artistic purpose rather shock people make money shown pornographic theater america curiousyellow good film anyone wanting study meat potato pun intended swedish cinema really film doesnt much plot\n",
      "\n",
      "Successfully processed 25000 examples.\n"
     ]
    }
   ],
   "source": [
    "# --- NLTK Setup ---\n",
    "print(\"Downloading NLTK data...\")\n",
    "nltk.download('punkt', quiet=True)       # For tokenization\n",
    "nltk.download('stopwords', quiet=True)   # For stop words\n",
    "nltk.download('wordnet', quiet=True)     # For lemmatization\n",
    "nltk.download('omw-1.4', quiet=True)     # For lemmatization\n",
    "nltk.download('punkt_tab')\n",
    "print(\"NLTK data downloaded.\")\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(example):\n",
    "    \"\"\"\n",
    "    Applies a full preprocessing pipeline to a single text example.\n",
    "    \"\"\"\n",
    "    # 'text' is the column name in the imdb dataset\n",
    "    text = example['text']\n",
    "    \n",
    "    # 1. Remove HTML tags and URLs\n",
    "    text = re.sub(r'<[^>]+>', ' ', text) # This regex removes HTML tags like <br />\n",
    "    text = re.sub(r'http\\S+|https\\S+', '', text) # This regex removes URLs\n",
    "    \n",
    "    text = text.lower() # Convert to lowercase\n",
    "    text = re.sub(r'[^a-z\\s]', '', text) # Remove punctuation and special characters (keep only letters and spaces)\n",
    "    \n",
    "    tokens = word_tokenize(text) # Splits the text into a list of words\n",
    "    tokens = [word for word in tokens if word not in stop_words] # Remove stop words\n",
    "    \n",
    "    # --- 6. Apply Stemming OR Lemmatization\n",
    "    # You generally do one or the other, not both.\n",
    "    # Lemmatization is usually preferred as it produces real words.\n",
    "    \n",
    "    # Option A: Stemming\n",
    "    # processed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    # Option B: Lemmatization (default)\n",
    "    processed_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # 7. Join tokens back into a single string\n",
    "    text = \" \".join(processed_tokens)\n",
    "    \n",
    "    # Update the example object\n",
    "    example['text'] = text\n",
    "    return example\n",
    "\n",
    "# Load the dataset\n",
    "print(\"Loading dataset...\")\n",
    "# We load 'train' split and take just the first 1000 examples for a quick demo\n",
    "# Remove `split='train[:1000]'` to run on the full dataset (will take time)\n",
    "dataset = load_dataset(\"stanfordnlp/imdb\", split='train[:50000]')\n",
    "    \n",
    "# 2. Show an example *before* processing\n",
    "print(\"\\n--- BEFORE PREPROCESSING ---\")\n",
    "print(dataset[0]['text'])\n",
    "    \n",
    "# 3. Apply the preprocessing function using .map()\n",
    "# This is the most efficient way and applies the function to every example\n",
    "print(\"\\nApplying preprocessing to dataset...\")\n",
    "processed_dataset = dataset.map(preprocess_text)\n",
    "    \n",
    "# 4. Show the same example *after* processing\n",
    "print(\"\\n--- AFTER PREPROCESSING ---\")\n",
    "print(processed_dataset[0]['text'])\n",
    "    \n",
    "print(f\"\\nSuccessfully processed {len(processed_dataset)} examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c7ead9-cd2d-44e4-be3b-fd6a97715704",
   "metadata": {},
   "source": [
    "#### The above shows the before and after of the text once we have completed preprocessing, we have converted all the words to lowercase so that same words in different cases are not considered different by the model, then we have removed stop words (like I, the, a etc) as they dont technically offer any information to make use of. We have also removed any URLS, HTML tags and special characters present and have converted each word to its lemma so that we can sort of reduce the number of words with same meaning in our vocabulary to make it more compact\n",
    "\n",
    "#### Now the below code is to apply text processing methods like Bag of Words and TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d667c02c-51c8-4757-b1bc-db9263143546",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Applying Bag of Words (CountVectorizer) ---\n",
      "BoW Matrix Shape: (25000, 5000)\n",
      "First 20 BoW features: ['abandoned' 'abc' 'ability' 'able' 'abraham' 'abrupt' 'absence' 'absent'\n",
      " 'absolute' 'absolutely' 'absurd' 'absurdity' 'abuse' 'abused' 'abusive'\n",
      " 'abysmal' 'academy' 'accent' 'accept' 'acceptable']\n",
      "\n",
      "BoW representation of the first document (word_index, count):\n",
      "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
      "\twith 101 stored elements and shape (1, 5000)>\n",
      "  Coords\tValues\n",
      "  (0, 3630)\t1\n",
      "  (0, 4754)\t1\n",
      "  (0, 4233)\t1\n",
      "  (0, 4349)\t1\n",
      "  (0, 1687)\t2\n",
      "  (0, 3599)\t1\n",
      "  (0, 145)\t1\n",
      "  (0, 2024)\t1\n",
      "  (0, 1493)\t1\n",
      "  (0, 4600)\t1\n",
      "  (0, 1449)\t1\n",
      "  (0, 975)\t1\n",
      "  (0, 4464)\t1\n",
      "  (0, 1601)\t1\n",
      "  (0, 1667)\t5\n",
      "  (0, 906)\t2\n",
      "  (0, 935)\t1\n",
      "  (0, 3540)\t3\n",
      "  (0, 3870)\t1\n",
      "  (0, 3281)\t2\n",
      "  (0, 239)\t1\n",
      "  (0, 4991)\t1\n",
      "  (0, 4366)\t3\n",
      "  (0, 1301)\t2\n",
      "  (0, 4266)\t1\n",
      "  :\t:\n",
      "  (0, 1889)\t2\n",
      "  (0, 3063)\t1\n",
      "  (0, 504)\t1\n",
      "  (0, 2364)\t1\n",
      "  (0, 1736)\t1\n",
      "  (0, 1670)\t1\n",
      "  (0, 1577)\t1\n",
      "  (0, 3981)\t3\n",
      "  (0, 251)\t1\n",
      "  (0, 3459)\t1\n",
      "  (0, 3513)\t1\n",
      "  (0, 3959)\t1\n",
      "  (0, 3197)\t1\n",
      "  (0, 2674)\t1\n",
      "  (0, 2850)\t1\n",
      "  (0, 4458)\t1\n",
      "  (0, 160)\t1\n",
      "  (0, 203)\t1\n",
      "  (0, 4813)\t1\n",
      "  (0, 4268)\t1\n",
      "  (0, 2747)\t1\n",
      "  (0, 3449)\t1\n",
      "  (0, 2268)\t1\n",
      "  (0, 1272)\t1\n",
      "  (0, 2888)\t1\n",
      "\n",
      "--- Applying TF-IDF (TfidfVectorizer) ---\n",
      "TF-IDF Matrix Shape: (25000, 5000)\n",
      "First 20 TF-IDF features: ['abandoned' 'abc' 'ability' 'able' 'abraham' 'abrupt' 'absence' 'absent'\n",
      " 'absolute' 'absolutely' 'absurd' 'absurdity' 'abuse' 'abused' 'abusive'\n",
      " 'abysmal' 'academy' 'accent' 'accept' 'acceptable']\n",
      "\n",
      "TF-IDF representation of the first document (word_index, tf-idf_score):\n",
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 101 stored elements and shape (1, 5000)>\n",
      "  Coords\tValues\n",
      "  (0, 3630)\t0.09052927975235954\n",
      "  (0, 4754)\t0.0664763296767417\n",
      "  (0, 4233)\t0.08307573768096306\n",
      "  (0, 4349)\t0.10572914998110353\n",
      "  (0, 1687)\t0.08054779215659962\n",
      "  (0, 3599)\t0.0736552856970242\n",
      "  (0, 145)\t0.040002374457796336\n",
      "  (0, 2024)\t0.07119078693517009\n",
      "  (0, 1493)\t0.04522469649993748\n",
      "  (0, 4600)\t0.077549297591786\n",
      "  (0, 1449)\t0.10059880914485561\n",
      "  (0, 975)\t0.0740244940687967\n",
      "  (0, 4464)\t0.09111121344129287\n",
      "  (0, 1601)\t0.05504890174492169\n",
      "  (0, 1667)\t0.13081375970869755\n",
      "  (0, 906)\t0.16997809281042028\n",
      "  (0, 935)\t0.10611594471316613\n",
      "  (0, 3540)\t0.11134504545783322\n",
      "  (0, 3870)\t0.03574018115256746\n",
      "  (0, 3281)\t0.08805639550048164\n",
      "  (0, 239)\t0.053191591232597514\n",
      "  (0, 4991)\t0.054674603191212555\n",
      "  (0, 4366)\t0.3491115247414566\n",
      "  (0, 1301)\t0.13701253427644933\n",
      "  (0, 4266)\t0.08144769166463668\n",
      "  :\t:\n",
      "  (0, 1889)\t0.06674227242920637\n",
      "  (0, 3063)\t0.05177496964830222\n",
      "  (0, 504)\t0.06445012086571415\n",
      "  (0, 2364)\t0.06390069601807646\n",
      "  (0, 1736)\t0.10335411721602217\n",
      "  (0, 1670)\t0.07464470101773742\n",
      "  (0, 1577)\t0.0524107430674139\n",
      "  (0, 3981)\t0.2216041704462205\n",
      "  (0, 251)\t0.09205179406560407\n",
      "  (0, 3459)\t0.0836204838861438\n",
      "  (0, 3513)\t0.05771051429081669\n",
      "  (0, 3959)\t0.08838283749182708\n",
      "  (0, 3197)\t0.04051169588066027\n",
      "  (0, 2674)\t0.03565005580440977\n",
      "  (0, 2850)\t0.060343974128613774\n",
      "  (0, 4458)\t0.07346414762915031\n",
      "  (0, 160)\t0.07979589142725516\n",
      "  (0, 203)\t0.05756107509456554\n",
      "  (0, 4813)\t0.09298741169920034\n",
      "  (0, 4268)\t0.0928080050827619\n",
      "  (0, 2747)\t0.10925922777098492\n",
      "  (0, 3449)\t0.1100628742866322\n",
      "  (0, 2268)\t0.08856711938279097\n",
      "  (0, 1272)\t0.04956672608737769\n",
      "  (0, 2888)\t0.038456271609565065\n",
      "\n",
      "Vectorization complete. You can now use 'bow_matrix' or 'tfidf_matrix' to train a model.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "documents = processed_dataset['text']\n",
    "labels = processed_dataset['label']\n",
    "\n",
    "# --- 4. Apply Bag of Words (BoW) ---\n",
    "print(\"\\n--- Applying Bag of Words (CountVectorizer) ---\")\n",
    "    \n",
    "# Initialize the vectorizer\n",
    "bow_vectorizer = CountVectorizer(max_features=5000) # max_features=5000 means it will only keep the 5000 most common words\n",
    "bow_matrix = bow_vectorizer.fit_transform(documents) # Fit the vectorizer to the data and transform the data into a matrix\n",
    "print(f\"BoW Matrix Shape: {bow_matrix.shape}\") \n",
    "# bow_matrix is a \"sparse matrix\" of shape (num_documents, max_features), here the rows represent each of the documents\n",
    "# and the columns represent the words and the value in each cell represents the number of times that word has occured in the document\n",
    "    \n",
    "feature_names_bow = bow_vectorizer.get_feature_names_out() # Show some of the vocabulary (features)\n",
    "print(f\"First 20 BoW features: {feature_names_bow[:20]}\")\n",
    "    \n",
    "print(\"\\nBoW representation of the first document (word_index, count):\") # Show the BoW representation of the first document\n",
    "print(bow_matrix[0])\n",
    "    \n",
    "# --- 5. Apply TF-IDF ---\n",
    "print(\"\\n--- Applying TF-IDF (TfidfVectorizer) ---\")\n",
    "    \n",
    "# Initialize the vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents) # Fit and transform\n",
    "print(f\"TF-IDF Matrix Shape: {tfidf_matrix.shape}\") # tfidf_matrix is also a sparse matrix of the same shape\n",
    "    \n",
    "# The features (vocabulary) will be the same as BoW if max_features is the same\n",
    "feature_names_tfidf = tfidf_vectorizer.get_feature_names_out()\n",
    "print(f\"First 20 TF-IDF features: {feature_names_tfidf[:20]}\")\n",
    "    \n",
    "# Show the TF-IDF representation of the first document\n",
    "print(\"\\nTF-IDF representation of the first document (word_index, tf-idf_score):\")\n",
    "print(tfidf_matrix[0])\n",
    "    \n",
    "print(\"\\nVectorization complete. You can now use 'bow_matrix' or 'tfidf_matrix' to train a model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bab28fb-6abb-4fd4-96aa-d02873132b74",
   "metadata": {},
   "source": [
    "#### In the above output, the Coords represent the coordinate of each cell in the matrix with the first index representing the row ie the document number and the second index representing the column ie the feature of the BoW and the value represents how many time it appears in BoW matrix while for TF-IDF matrix, it represents the TF-IDF value and the higher the value, the better is the word at discriminating (and the word is more important for that document sort of) that documents from others\n",
    "\n",
    "#### Now the below code is for label encoding, embedding and Word2Vec and we require tokens seperately not just documents, hence re running the preprocessing steps and the other steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d082b8c-e73a-4a3a-81c6-c8b43e4f759c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "\n",
      "--- BEFORE PREPROCESSING ---\n",
      "I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn't have much of a plot.\n",
      "\n",
      "Applying preprocessing to dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "696eba4262444edba0a35f87beb0d0f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- AFTER PREPROCESSING ---\n",
      "['rented', 'curiousyellow', 'video', 'store', 'controversy', 'surrounded', 'first', 'released', 'also', 'heard', 'first', 'seized', 'u', 'custom', 'ever', 'tried', 'enter', 'country', 'therefore', 'fan', 'film', 'considered', 'controversial', 'really', 'see', 'plot', 'centered', 'around', 'young', 'swedish', 'drama', 'student', 'named', 'lena', 'want', 'learn', 'everything', 'life', 'particular', 'want', 'focus', 'attention', 'making', 'sort', 'documentary', 'average', 'swede', 'thought', 'certain', 'political', 'issue', 'vietnam', 'war', 'race', 'issue', 'united', 'state', 'asking', 'politician', 'ordinary', 'denizen', 'stockholm', 'opinion', 'politics', 'sex', 'drama', 'teacher', 'classmate', 'married', 'men', 'kill', 'curiousyellow', 'year', 'ago', 'considered', 'pornographic', 'really', 'sex', 'nudity', 'scene', 'far', 'even', 'shot', 'like', 'cheaply', 'made', 'porno', 'countryman', 'mind', 'find', 'shocking', 'reality', 'sex', 'nudity', 'major', 'staple', 'swedish', 'cinema', 'even', 'ingmar', 'bergman', 'arguably', 'answer', 'good', 'old', 'boy', 'john', 'ford', 'sex', 'scene', 'film', 'commend', 'filmmaker', 'fact', 'sex', 'shown', 'film', 'shown', 'artistic', 'purpose', 'rather', 'shock', 'people', 'make', 'money', 'shown', 'pornographic', 'theater', 'america', 'curiousyellow', 'good', 'film', 'anyone', 'wanting', 'study', 'meat', 'potato', 'pun', 'intended', 'swedish', 'cinema', 'really', 'film', 'doesnt', 'much', 'plot']\n",
      "\n",
      "Successfully processed 25000 examples.\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(example):\n",
    "    \"\"\"\n",
    "    Applies a full preprocessing pipeline to a single text example.\n",
    "    \"\"\"\n",
    "    # 'text' is the column name in the imdb dataset\n",
    "    text = example['text']\n",
    "    \n",
    "    # 1. Remove HTML tags and URLs\n",
    "    text = re.sub(r'<[^>]+>', ' ', text) # This regex removes HTML tags like <br />\n",
    "    text = re.sub(r'http\\S+|https\\S+', '', text) # This regex removes URLs\n",
    "    \n",
    "    text = text.lower() # Convert to lowercase\n",
    "    text = re.sub(r'[^a-z\\s]', '', text) # Remove punctuation and special characters (keep only letters and spaces)\n",
    "    \n",
    "    tokens = word_tokenize(text) # Splits the text into a list of words\n",
    "    tokens = [word for word in tokens if word not in stop_words] # Remove stop words\n",
    "    \n",
    "    # --- 6. Apply Stemming OR Lemmatization\n",
    "    # You generally do one or the other, not both.\n",
    "    # Lemmatization is usually preferred as it produces real words.\n",
    "    \n",
    "    # Option A: Stemming\n",
    "    # processed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    # Option B: Lemmatization (default)\n",
    "    processed_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # Update the example object\n",
    "    example['tokens'] = processed_tokens\n",
    "    return example\n",
    "\n",
    "# Load the dataset\n",
    "print(\"Loading dataset...\")\n",
    "# We load 'train' split and take just the first 1000 examples for a quick demo\n",
    "# Remove `split='train[:1000]'` to run on the full dataset (will take time)\n",
    "dataset = load_dataset(\"stanfordnlp/imdb\", split='train[:50000]')\n",
    "    \n",
    "# 2. Show an example *before* processing\n",
    "print(\"\\n--- BEFORE PREPROCESSING ---\")\n",
    "print(dataset[0]['text'])\n",
    "    \n",
    "# 3. Apply the preprocessing function using .map()\n",
    "# This is the most efficient way and applies the function to every example\n",
    "print(\"\\nApplying preprocessing to dataset...\")\n",
    "processed_dataset = dataset.map(preprocess_text)\n",
    "    \n",
    "# 4. Show the same example *after* processing\n",
    "print(\"\\n--- AFTER PREPROCESSING ---\")\n",
    "print(processed_dataset[0]['tokens'])\n",
    "    \n",
    "print(f\"\\nSuccessfully processed {len(processed_dataset)} examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2960a55b-af26-49fc-8187-a01659aea50e",
   "metadata": {},
   "source": [
    "#### The above code is to preporcess so as to prepare to apply Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "af91bec1-ee47-47d6-a054-0e71642eda19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training CBOW Model (sg=0) ---\n",
      "CBOW Model trained and saved as 'cbow_word2vec.model'.\n",
      "Example: Words most similar to 'movie' (CBOW):\n",
      "[('film', 0.7948123812675476), ('flick', 0.6767253279685974), ('sequel', 0.6639348864555359), ('anyway', 0.5959745645523071), ('opinion', 0.5786111354827881), ('suppose', 0.5699934959411621), ('sucked', 0.5672647953033447), ('stuff', 0.563139796257019), ('honestly', 0.5620121955871582), ('sleepfest', 0.5588194727897644)]\n",
      "\n",
      "--- Training Skip-gram Model (sg=1) ---\n",
      "Skip-gram Model trained and saved as 'skipgram_word2vec.model'.\n",
      "Example: Words most similar to 'film' (Skip-gram):\n",
      "[('movie', 0.9008914828300476), ('bilal', 0.7973375916481018), ('biopics', 0.7901825308799744), ('horrorthriller', 0.7898569703102112), ('weir', 0.7880809307098389), ('catiii', 0.7864723801612854), ('categorized', 0.7796862125396729), ('filmgoers', 0.7780632376670837), ('malfique', 0.7770559787750244), ('romanticcomedy', 0.7767753601074219)]\n",
      "\n",
      "Vectorization complete.\n"
     ]
    }
   ],
   "source": [
    "import gensim.models\n",
    "corpus = processed_dataset['tokens']\n",
    "\n",
    "print(\"\\n--- Training CBOW Model (sg=0) ---\")\n",
    "\n",
    "cbow_model = gensim.models.Word2Vec( # Initialize and train the Word2Vec model\n",
    "    sentences=corpus,\n",
    "    vector_size=100, # vector_size=100: Creates 100-dimension word embeddings\n",
    "    window=5, # window=5: Considers 5 words before and 5 words after the target word\n",
    "    min_count=2, # min_count=2: Ignores all words with a total frequency lower than 2\n",
    "    sg=0, # sg=0: This is the flag that specifies CBOW\n",
    "    workers=4 # workers=4: Use 4 CPU cores to speed up training\n",
    ")\n",
    "\n",
    "cbow_model.save(\"cbow_word2vec.model\") # Save the model for later use\n",
    "print(\"CBOW Model trained and saved as 'cbow_word2vec.model'.\")\n",
    "    \n",
    "# Show example results\n",
    "print(\"Example: Words most similar to 'movie' (CBOW):\")\n",
    "try:\n",
    "    # wv.most_similar() finds the k-nearest neighbors\n",
    "    print(cbow_model.wv.most_similar('movie'))\n",
    "except KeyError:\n",
    "    print(\"'movie' not in vocabulary (or filtered by min_count).\")\n",
    "    \n",
    "# --- 5. Apply Skip-gram ---\n",
    "print(\"\\n--- Training Skip-gram Model (sg=1) ---\")\n",
    "    \n",
    "# The parameters are the same, but we change 'sg=1'\n",
    "skipgram_model = gensim.models.Word2Vec(\n",
    "    sentences=corpus,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    sg=1, # sg=1: This is the flag that specifies Skip-gram\n",
    "    workers=4\n",
    ")\n",
    "\n",
    "skipgram_model.save(\"skipgram_word2vec.model\") # Save the model\n",
    "print(\"Skip-gram Model trained and saved as 'skipgram_word2vec.model'.\")\n",
    "\n",
    "# Show example results\n",
    "print(\"Example: Words most similar to 'film' (Skip-gram):\")\n",
    "try:\n",
    "    print(skipgram_model.wv.most_similar('film'))\n",
    "except KeyError:\n",
    "    print(\"'film' not in vocabulary (or filtered by min_count).\")\n",
    "        \n",
    "print(\"\\nVectorization complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e7adaf-9592-445f-b778-b3bf99388d14",
   "metadata": {},
   "source": [
    "#### For 2 words to be considered similar in CBOW, when the window is smaller, they should sort of fit in the same sentence literally and subjectively as the window gets bigger and bigger, we incorporate more semantic meaning into as well and then when the words have to be considered similar then they should be meaningfully same as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3036e0cd-74d2-415a-afbb-6f5db1b42691",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
